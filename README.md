# Databricks MCP Server
A production-ready MCP server that exposes 50+ Databricks workspace operations through the MCP for AI agents like Cursor, Claude Desktop, and more.

> **âš ï¸ DISCLAIMER**: This is **NOT** an official Databricks product and is **NOT** supported by Databricks. This is a community project that uses the Databricks SDK to provide MCP integration. Use at your own risk. For official Databricks support, please contact Databricks directly.

## ğŸš€ Quick Start (5 minutes)

### 1. Set Your Credentials

```bash
export DATABRICKS_HOST="https://your-workspace.cloud.databricks.com"
export DATABRICKS_TOKEN="dapi..."  # Your Personal Access Token
```

### 2. Start the Server

```bash
cd databricks_cli_mcp
./start_local_mcp.sh
```

Server runs at: `http://localhost:8080/mcp`

### 3. Configure Cursor

Add to Cursor MCP settings (`~/.cursor/mcp.json`):

```json
{
  "mcpServers": {
    "databricks": {
      "url": "http://localhost:8080/mcp"
    }
  }
}
```

### 4. Start Using!

In Cursor, try:
```
"List my Databricks clusters"
"Show me my SQL warehouses"
"Execute query: SELECT * FROM samples.nyctaxi.trips LIMIT 10"
```

---

## ğŸ“¦ What You Get

### 50 Databricks Tools

- **Clusters** (7): Create, start, stop, list, manage
- **Jobs** (6): Create, run, cancel, monitor
- **SQL** (8): Execute queries, manage warehouses
- **Unity Catalog** (11): Catalogs, schemas, tables, volumes
- **Workspace** (6): Files, notebooks, directories
- **Notebooks** (5): Import, export, run
- **Repos** (5): Git integration
- **Secrets** (4): Secret management
- **Tasks** (2): Async operations

---

## ğŸ—ï¸ Project Structure

```
databricks_cli_mcp/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ server.py              # Main MCP server (FastMCP)
â”œâ”€â”€ local_mcp_server.py    # Local server with PAT auth
â”œâ”€â”€ start_local_mcp.sh     # Quick start script
â”‚
â”œâ”€â”€ auth.py                # Authentication logic
â”œâ”€â”€ databricks_client.py   # Databricks SDK wrapper
â”œâ”€â”€ task_manager.py        # Async task management
â”œâ”€â”€ tool_registry.py       # Tool schema converter
â”‚
â”œâ”€â”€ tools/                 # 50 Databricks tools
â”‚   â”œâ”€â”€ clusters.py
â”‚   â”œâ”€â”€ jobs.py
â”‚   â”œâ”€â”€ sql.py
â”‚   â”œâ”€â”€ unity_catalog.py
â”‚   â”œâ”€â”€ workspace.py
â”‚   â”œâ”€â”€ notebooks.py
â”‚   â”œâ”€â”€ repos.py
â”‚   â””â”€â”€ secrets.py
â”‚
â”œâ”€â”€ app.yaml               # Databricks Apps deployment config
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ pyproject.toml         # Package configuration
```

---

## ğŸ” Authentication

### Option 1: PAT (Personal Access Token) - Recommended

**Best for**: Development, personal use

```bash
# Get your PAT from Databricks UI
# Settings â†’ Developer â†’ Access tokens â†’ Generate new token

export DATABRICKS_HOST="https://your-workspace.cloud.databricks.com"
export DATABRICKS_TOKEN="dapi..."

./start_local_mcp.sh
```

**Pros**: âœ… Simple, âœ… Quick, âœ… Works immediately  
**Cons**: âš ï¸ 90-day expiry, âš ï¸ Manual renewal

### Option 2: M2M OAuth (Advanced)

**Best for**: Production, teams, CI/CD

1. Create service principal in Databricks
2. Generate OAuth secret
3. Set environment variables:

```bash
export DATABRICKS_APP_URL="https://your-app.databricksapps.com"
export DATABRICKS_HOST="https://your-workspace.cloud.databricks.com"
export DATABRICKS_CLIENT_ID="<service-principal-id>"
export DATABRICKS_CLIENT_SECRET="<oauth-secret>"

./start_oauth_proxy.sh
```

**Pros**: âœ… Auto-refresh, âœ… Better security, âœ… Team sharing  
**Cons**: âš ï¸ Requires service principal setup

---

## â˜ï¸ Deploy to Databricks Apps

### 1. Authenticate

```bash
databricks auth login
```

### 2. Deploy

```bash
./deploy_correct.sh
```

### 3. Get Your App URL

```bash
databricks apps get databricks-mcp-server | jq -r .url
```

---

## ğŸ§ª Testing

### Test Locally

```bash
python test_local.py
```

### Test from Notebook

Upload `notebooks/simple_test.py` to Databricks and run it.

---

## ğŸ’¡ Example Usage

### In Cursor

```
"Create a cluster named 'test' with 2 workers"
"Start cluster 0730-172948-runts698"
"Execute SQL: SELECT current_database()"
"List tables in catalog main"
"Export notebook /Users/me/analysis"
"Show me my job runs"
```

### Programmatically

```python
from databricks.sdk import WorkspaceClient

client = WorkspaceClient()

# List clusters
clusters = list(client.clusters.list())
print(f"Found {len(clusters)} clusters")

# Execute SQL
result = client.sql.execute_query(
    warehouse_id="abc123",
    query="SELECT * FROM samples.nyctaxi.trips LIMIT 10"
)
```

---

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `DATABRICKS_HOST` | Workspace URL | âœ… Yes |
| `DATABRICKS_TOKEN` | Personal Access Token | âœ… Yes (PAT mode) |
| `DATABRICKS_CLIENT_ID` | Service principal ID | For OAuth |
| `DATABRICKS_CLIENT_SECRET` | OAuth secret | For OAuth |
| `DATABRICKS_APP_URL` | Deployed app URL | For OAuth |

### Server Configuration

The server runs on port `8080` by default. Change in `start_local_mcp.sh`:

```bash
python3 local_mcp_server.py --port 8080
```

---

## ğŸ› Troubleshooting

### Server won't start

```bash
# Check if port is in use
lsof -i :8080

# Kill existing process
kill -9 <PID>

# Restart
./start_local_mcp.sh
```

### Authentication failed

```bash
# Verify credentials
echo $DATABRICKS_HOST
echo $DATABRICKS_TOKEN

# Test manually
curl -H "Authorization: Bearer $DATABRICKS_TOKEN" \
  $DATABRICKS_HOST/api/2.0/clusters/list
```

### Cursor not connecting

1. Check server is running: `curl http://localhost:8080/health`
2. Verify Cursor config: `cat ~/.cursor/mcp.json`
3. Restart Cursor
4. Check logs: `tail -f mcp_server.log`

### Permission errors

- **Can't create clusters?** Ask your admin for permissions
- **Can't access catalogs?** Check Unity Catalog permissions
- **Can't run queries?** Verify SQL warehouse access

---

## ğŸ“š Key Concepts

### MCP (Model Context Protocol)

An open protocol that lets AI assistants connect to external data sources and tools. Think of it as "API for AIs".

### FastMCP

A Python framework for building MCP servers. Makes it easy to expose Python functions as tools for AI agents.

### Databricks SDK

Official Python library for interacting with Databricks APIs. Powers all the tools in this server.

---

## ğŸ”— Resources

- **Databricks API Docs**: https://docs.databricks.com/api/
- **MCP Protocol**: https://modelcontextprotocol.io/
- **FastMCP**: https://github.com/jlowin/fastmcp
- **Databricks SDK**: https://github.com/databricks/databricks-sdk-py

---

## ğŸ“ Requirements

- Python 3.11+
- Databricks workspace with API access
- Personal Access Token or Service Principal
- Cursor IDE (or any MCP-compatible client)

---

## ğŸ¯ Common Tasks

### Start a cluster

```python
# In Cursor
"Start cluster <cluster-id>"

# Or directly
from databricks.sdk import WorkspaceClient
client = WorkspaceClient()
client.clusters.start(cluster_id="...")
```

### Run a SQL query

```python
# In Cursor
"Execute query: SELECT * FROM my_table LIMIT 10"

# Or directly
result = client.sql.execute_query(
    warehouse_id="...",
    query="SELECT * FROM my_table"
)
```

### Create a job

```python
# In Cursor
"Create a job that runs notebook /path/to/notebook daily"

# Or use the Databricks UI - easier for complex jobs!
```

---

## âš™ï¸ Advanced Configuration

### Custom Tools

Add your own tools in `tools/` directory:

```python
# tools/custom.py
from fastmcp import Context

@mcp.tool()
async def my_custom_tool(param: str, context: Context):
    """My custom Databricks operation."""
    client = get_databricks_client(context)
    # Your logic here
    return {"result": "success"}
```

Then register in `server.py`:

```python
from tools import custom
```

### Production Deployment

For production use:
1. Use M2M OAuth (not PAT)
2. Deploy to Databricks Apps
3. Enable monitoring (`/metrics` endpoint)
4. Set up proper logging
5. Configure auto-scaling

---

## ğŸ‰ Success!

You now have a fully functional Databricks MCP server! 

**Next steps**:
1. âœ… Start the server
2. âœ… Configure Cursor  
3. âœ… Try listing your clusters
4. âœ… Execute your first SQL query
5. ğŸš€ Automate all the things!

---

## ğŸ“„ License

MIT

## ğŸ¤ Contributing

Issues and PRs welcome! This project follows standard GitHub workflow.

## ğŸ’¬ Support

- Check logs: `tail -f mcp_server.log`
- Review error messages in Cursor
- Verify Databricks permissions
- Test API access directly with `curl`

---

**Built with â¤ï¸ for Databricks automation**
