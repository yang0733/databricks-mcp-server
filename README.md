# Databricks CLI MCP Server

A stateful MCP (Model Context Protocol) server that exposes comprehensive Databricks CLI capabilities via HTTP API for agent-driven automation.

## ğŸ¯ Features

- **Comprehensive Coverage**: 43+ tools across 8 Databricks services
- **Stateful Context**: Maintains session state across requests (workspace paths, current cluster/job/warehouse)
- **HTTP API**: Programmatic access for AI agents and automation scripts
- **Secure Authentication**: Per-request authentication via user personal access tokens
- **Production Ready**: Deployable as Databricks App for team-wide access

## ğŸ“¦ Supported Operations

### Clusters (6 tools)
- Create, start, stop, delete clusters
- List all clusters
- Get detailed cluster information

### Jobs (7 tools)
- Create and configure jobs
- Run jobs with parameters
- List, get, cancel, and delete jobs
- Get run status and results

### Notebooks (4 tools)
- Import/export notebooks
- List workspace notebooks
- Run notebooks on clusters

### Workspace (5 tools)
- List workspace objects
- Import/export files
- Delete paths
- Create directories

### Repos (5 tools)
- Link Git repositories
- Update to branches/tags
- List and manage repos

### Secrets (5 tools)
- Manage secret scopes
- Store and list secrets
- Secure credential management

### SQL Warehouses (5 tools)
- List, start, stop warehouses
- Execute SQL queries
- Retrieve query results

### Unity Catalog (6 tools)
- Browse catalogs, schemas, tables
- Get table metadata
- Manage volumes

## ğŸš€ Quick Start

### Prerequisites

```bash
# Python 3.8 or higher
python --version

# Install dependencies
pip install -r requirements.txt
```

### Local Testing

1. **Update test credentials** in `test_local.py`:
   ```python
   DATABRICKS_HOST = "https://your-workspace.cloud.databricks.com"
   DATABRICKS_TOKEN = "your-personal-access-token"
   ```

2. **Start the server**:
   ```bash
   python server.py
   # Server starts on http://localhost:8000
   ```

3. **Run tests** (in another terminal):
   ```bash
   python test_local.py
   ```

### Getting a Personal Access Token

1. Navigate to your Databricks workspace
2. Click your username â†’ **Settings**
3. Go to **Developer** â†’ **Access tokens**
4. Click **Generate new token**
5. Copy the token (you won't see it again!)

## ğŸ”§ Usage Examples

### Using the MCP Client

```python
import asyncio
from fastmcp.client import Client
from fastmcp.client.transports import StreamableHttpTransport

async def example():
    headers = {
        "x-databricks-host": "https://my-workspace.cloud.databricks.com",
        "x-databricks-token": "dapi...",
        "x-session-id": "my-session-123"
    }
    
    transport = StreamableHttpTransport("http://localhost:8000/mcp/", headers=headers)
    
    async with Client(transport=transport) as client:
        # List clusters
        result = await client.call_tool("list_clusters", {})
        print(result[0].content[0].text)
        
        # Set workspace context
        await client.call_tool("set_workspace_path", {
            "path": "/Workspace/Users/me"
        })
        
        # List notebooks (uses context path)
        result = await client.call_tool("list_notebooks", {
            "path": ".",  # Relative to context
            "recursive": True
        })
        print(result[0].content[0].text)

asyncio.run(example())
```

### Using cURL

```bash
curl -X POST http://localhost:8000/mcp/ \
  -H "Content-Type: application/json" \
  -H "x-databricks-host: https://my-workspace.cloud.databricks.com" \
  -H "x-databricks-token: dapi..." \
  -H "x-session-id: my-session" \
  -d '{
    "method": "tools/call",
    "params": {
      "name": "list_clusters",
      "arguments": {}
    }
  }'
```

## ğŸ”„ Stateful Context

The server maintains session state to simplify multi-step operations:

```python
# Set context once
await client.call_tool("set_workspace_path", {"path": "/Workspace/Users/me"})
await client.call_tool("set_current_cluster", {"cluster_id": "1234-567890-abc123"})

# Use context in subsequent calls
await client.call_tool("list_notebooks", {"path": "."})  # Uses workspace path
await client.call_tool("start_cluster", {})  # Uses current cluster
await client.call_tool("run_notebook", {
    "path": "MyNotebook"  # Relative to workspace path
    # cluster_id not needed - uses current cluster
})
```

### Context Management Tools

- `get_session_context()` - View current session state
- `set_workspace_path(path)` - Set base workspace path
- `set_current_cluster(cluster_id)` - Set default cluster
- `set_current_warehouse(warehouse_id)` - Set default warehouse
- `clear_session_context()` - Reset session state

## ğŸ“š Complete Tool Reference

### Clusters

```python
# Create cluster
create_cluster(
    cluster_name="my-cluster",
    spark_version="13.3.x-scala2.12",
    node_type_id="i3.xlarge",
    num_workers=2
)

# Start/stop cluster
start_cluster(cluster_id="...")
stop_cluster(cluster_id="...")

# Delete cluster
delete_cluster(cluster_id="...")

# List and get info
list_clusters()
get_cluster(cluster_id="...")
```

### Jobs

```python
# Create job
create_job(
    job_name="my-job",
    tasks=[{
        "task_key": "main",
        "notebook_task": {
            "notebook_path": "/Workspace/MyNotebook",
            "source": "WORKSPACE"
        },
        "new_cluster": {...}
    }]
)

# Run job
run_job(job_id=123, notebook_params={"param1": "value1"})

# Manage jobs
list_jobs(limit=25)
get_job(job_id=123)
get_run(run_id=456)
cancel_run(run_id=456)
delete_job(job_id=123)
```

### SQL

```python
# Execute query
execute_query(
    query="SELECT * FROM my_table LIMIT 10",
    warehouse_id="abc123",
    wait_timeout="30s"
)

# Get results
get_query_results(statement_id="01234567-89ab-cdef-0123-456789abcdef")

# Manage warehouses
list_warehouses()
start_warehouse(warehouse_id="abc123")
stop_warehouse(warehouse_id="abc123")
```

## ğŸš€ Databricks App Deployment

Deploy the MCP server as a Databricks App for production use.

### Prerequisites

```bash
# Install Databricks CLI
brew install databricks

# Configure authentication
databricks configure --token
# Enter your workspace URL and personal access token
```

### Deployment Steps

1. **Navigate to project directory**:
   ```bash
   cd databricks_cli_mcp
   ```

2. **Deploy the app**:
   ```bash
   databricks apps deploy databricks-cli-mcp-server --source-path .
   ```

3. **Monitor deployment**:
   ```bash
   databricks apps logs databricks-cli-mcp-server --follow
   ```

4. **Check status**:
   ```bash
   databricks apps get databricks-cli-mcp-server
   ```

### Access the Deployed App

Once deployed, the app will be available at:
```
https://your-workspace.cloud.databricks.com/apps/databricks-cli-mcp-server
```

Agents can connect to the production endpoint:
```python
SERVER_URL = "https://your-workspace.cloud.databricks.com/apps/databricks-cli-mcp-server/mcp/"
```

### Managing the Deployment

```bash
# View logs
databricks apps logs databricks-cli-mcp-server --tail 100

# Restart app
databricks apps restart databricks-cli-mcp-server

# Update app (after code changes)
databricks apps deploy databricks-cli-mcp-server --source-path .

# Delete app
databricks apps delete databricks-cli-mcp-server
```

### Scaling

Edit `databricks.yml` to adjust compute size:

```yaml
compute:
  size: SMALL   # 1-10 users
  size: MEDIUM  # 10-50 users (default)
  size: LARGE   # 50+ users
```

## ğŸ” Security Considerations

### Authentication

- **Per-Request Authentication**: Each client provides their own Databricks credentials
- **No Shared Credentials**: Server doesn't store any authentication tokens
- **Personal Access Tokens**: Users authenticate with their own PATs
- **Header-Based**: Credentials passed via HTTP headers (`x-databricks-host`, `x-databricks-token`)

### Best Practices

1. **Rotate Tokens Regularly**: Generate new PATs every 90 days
2. **Limit Token Scope**: Use tokens with minimum required permissions
3. **Secure Transport**: Always use HTTPS in production
4. **Session Management**: Use unique session IDs per user/agent
5. **Audit Logging**: Enable Databricks audit logs to track API usage

## ğŸ›  Development

### Project Structure

```
databricks_cli_mcp/
â”œâ”€â”€ server.py              # Main MCP server
â”œâ”€â”€ auth.py                # Authentication handler
â”œâ”€â”€ databricks_client.py   # Client wrapper with context management
â”œâ”€â”€ tools/                 # Tool implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ clusters.py       # Cluster management
â”‚   â”œâ”€â”€ jobs.py           # Job management
â”‚   â”œâ”€â”€ notebooks.py      # Notebook operations
â”‚   â”œâ”€â”€ workspace.py      # Workspace file operations
â”‚   â”œâ”€â”€ repos.py          # Git repository integration
â”‚   â”œâ”€â”€ secrets.py        # Secrets management
â”‚   â”œâ”€â”€ sql.py            # SQL warehouse and queries
â”‚   â””â”€â”€ unity_catalog.py  # Unity Catalog operations
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ databricks.yml        # Databricks App config
â”œâ”€â”€ test_local.py         # Local testing script
â””â”€â”€ README.md             # This file
```

### Adding New Tools

1. Create or edit a tool module in `tools/`
2. Implement the tool function
3. Register it in the `register_tools()` function
4. Import and register in `server.py`

Example:

```python
# In tools/custom.py
def register_tools(mcp, get_wrapper):
    @mcp.tool()
    def my_custom_tool(param: str, context=None) -> dict:
        wrapper = get_wrapper(context)
        # Use wrapper.client for Databricks SDK calls
        # Use wrapper.context for stateful operations
        return {"result": "success"}

# In server.py
from tools import custom
custom.register_tools(mcp, get_databricks_wrapper)
```

## ğŸ“Š Performance

### Benchmarks (Local Testing)

- **Tool Call Latency**: 50-200ms (depends on Databricks API)
- **Concurrent Requests**: Supports 100+ simultaneous sessions
- **Memory Usage**: ~100MB base + ~5MB per active session

### Optimization Tips

1. **Reuse Sessions**: Use same `session-id` for related operations
2. **Batch Operations**: Combine multiple related tool calls
3. **Cache Context**: Set workspace path/cluster once, reuse many times
4. **Async Clients**: Use async/await for concurrent requests

## ğŸ› Troubleshooting

### Common Issues

**"Missing Databricks credentials"**
- Ensure `x-databricks-host` and `x-databricks-token` headers are set
- Verify token hasn't expired
- Check host URL format (must include `https://`)

**"No cluster_id provided and no current cluster set"**
- Either pass `cluster_id` parameter explicitly
- Or set current cluster: `set_current_cluster(cluster_id="...")`

**Connection timeout**
- Check server is running: `curl http://localhost:8000/health`
- Verify firewall settings
- Ensure Databricks workspace is accessible

**Authentication errors**
- Regenerate personal access token
- Verify workspace URL is correct
- Check user permissions in Databricks

## ğŸ¤ Contributing

This is an internal project. For improvements:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test locally with `test_local.py`
5. Submit a pull request

## ğŸ“„ License

Internal use only.

## ğŸ“ Support

For issues or questions:
- Check this README
- Review logs: `databricks apps logs databricks-cli-mcp-server`
- Contact the development team

---

**Built with FastMCP and Databricks SDK**

Last updated: October 2025

